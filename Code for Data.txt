import pandas as pd, numpy as np;
import matplotlib.pyplot as plt;
import seaborn as sns;
%matplotlib inline
import warnings;
warnings.filterwarnings("ignore");
import xgboost;
from sklearn.model_selection import train_test_split, cross_val_predict;
from sklearn.tree import DecisionTreeClassifier as DTC;
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score;
from sklearn.preprocessing import LabelEncoder;
from sklearn.preprocessing import MinMaxScaler;
from sklearn.neighbors import LocalOutlierFactor;
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import GridSearchCV; 

data = pd.read_csv('studentData.csv', sep=';')
data.head()

data.shape

data.isnull().sum()

data.info()

data.describe()

data.describe(include='object')

data.sample(10)

data.SelfRegulation

plt.figure(figsize= (7, 5))
plt.hist(x= data.Age, bins=20)
plt.xlabel('Ages of students')
plt.ylabel('Frequency of Students')
plt.title('Ages of Students Histogram')

plt.figure(figsize= (7, 5))
plt.hist(x= data.StudyEffort, bins=20)
plt.xlabel('Number of effort put in studying')
plt.ylabel('Frequency of Students')
plt.title('Effort put in Studying Histogram')

plt.figure(figsize= (7, 5))
plt.hist(x= data.StudyTime)
plt.xlabel('Number of study time')
plt.ylabel('Frequency of Students')
plt.title('StudyTime Histogram')

numeric_data = data[['StudyTime','StudyEffort', 'Age']]

data.plot.scatter(x = 'StudyEffort', y='Age')
plt.show()

plt.figure(figsize=(18, 14))
sns.countplot(x='Age', hue='Label', data=data)
plt.title('Age and Label')
plt.xlabel('Age')
plt.ylabel('Label')
plt.show()

plt.figure(figsize= (7, 5))
plt.hist(x= data.SelfRegulation, bins=20)
plt.xlabel('SelfRegulation of students')
plt.ylabel('Frequency of Students')
plt.title('SelfRegulation of Students Histogram')

sns.boxplot(data['SelfRegulation'])

plt.figure(figsize=(7, 5))
data['Modality'].value_counts().plot(kind='bar', color='skyblue')
plt.xlabel('Modality')
plt.ylabel('Frequency of Modality')
plt.title('Modality Bar Chart')
plt.show()

## separate the class label from regular attributes
y = data[['Label']]
X = data[data.columns.difference(['Label'])]
X.info()



baseline_sklearn_data = X.copy()
num_cols = baseline_sklearn_data.select_dtypes(include=['int64', 'float64']).columns
baseline_sklearn_data[num_cols] = baseline_sklearn_data[num_cols].apply(lambda col: col.fillna(0, axis=0))
obj_cols = baseline_sklearn_data.select_dtypes(include=['object', 'category']).columns
baseline_sklearn_data[obj_cols] = baseline_sklearn_data[obj_cols].apply(lambda col: col.fillna('?', axis=0))
baseline_sklearn_data[obj_cols] = baseline_sklearn_data[obj_cols].apply(LabelEncoder().fit_transform)
print(baseline_sklearn_data.info())
baseline_sklearn_data.sample(10)

## Use a ratio of 70% training - 30% testing
X_train, X_test, y_train, y_test = train_test_split(baseline_sklearn_data, y, test_size=0.3, random_state=1)
## Create a decision tree classifier
sk_dt = DTC()
## Train the decision tree model
sk_dt.fit(X_train, y_train)
## Make predictions on the test portion
predictions = sk_dt.predict(X_test)
## Check the predicted labels against the actual labels stored in y_test and output various scores
print(classification_report(y_test, predictions))
## Output the confusion matrix
cm = confusion_matrix(y_test, predictions)
print(cm)

Handling with missing
## drop the rows where with attributes missing less than 5%
data1 = data.copy();
data1 = data1.dropna(subset=['AcademicYear', 'ExtrinsicMotivation', 'GroupWork', 'IntrinsicMotivation',
                             'SelfEfficacy', 'StudyEffort', 'StudyTime', 'HighSchoolAverage', 'HighSchoolEnglish',
                             'HighSchoolMaths', 'Sex', 'Age', 'Discipline', 'Course']);
data1.info()
# Split validatiuon using ratio of 70% training - 30% testing
X_train, X_test, y_train, y_test = train_test_split(baseline_sklearn_data, y, test_size=0.3, random_state=1)
sk_dt = DTC()
## first train it on the train portion of X and y
sk_dt.fit(X_train, y_train)
# Make predictions on the test portion
predictions = sk_dt.predict(X_test)
report = classification_report(y_test, predictions)
print(report)
# Output the confusion matrix
cm = confusion_matrix(y_test, predictions)
print(cm)

## remove the SelfRegulation column
data1 = data1.drop(['SelfRegulation'], axis=1);
data1.info()
# Split validatiuon using ratio of 70% training - 30% testing
X_train, X_test, y_train, y_test = train_test_split(baseline_sklearn_data, y, test_size=0.3, random_state=1)
sk_dt = DTC()
## first train it on the train portion of X and y
sk_dt.fit(X_train, y_train)
# Make predictions on the test portion
predictions = sk_dt.predict(X_test)
report = classification_report(y_test, predictions)
print(report)
# Output the confusion matrix
cm = confusion_matrix(y_test, predictions)
print(cm)

# Try to replace Modality's missing data with mode
data10 = data1.copy()
mode_value = data10['Modality'].mode()[0]
data10['Modality'] = data10['Modality'].fillna(mode_value)

data10.Modality
# Split validatiuon using ratio of 70% training - 30% testing
X_train, X_test, y_train, y_test = train_test_split(baseline_sklearn_data, y, test_size=0.3, random_state=1)
sk_dt = DTC()
## first train it on the train portion of X and y
sk_dt.fit(X_train, y_train)
# Make predictions on the test portion
predictions = sk_dt.predict(X_test)
report = classification_report(y_test, predictions)
print(report)
# Output the confusion matrix
cm = confusion_matrix(y_test, predictions)
print(cm)

Removing outliers
num_cols = X.select_dtypes(include=['int64', 'float64']).columns
cat_cols = X.select_dtypes(include=['object', 'category']).columns
baseline_sklearn_data = X.copy()
baseline_sklearn_data[num_cols] = baseline_sklearn_data[num_cols].apply(lambda col: col.fillna(0, axis=0))
baseline_sklearn_data[cat_cols] = baseline_sklearn_data[cat_cols].apply(lambda col: col.fillna('?', axis=0))
baseline_sklearn_data[cat_cols] = baseline_sklearn_data[cat_cols].apply(LabelEncoder().fit_transform)

## Apply LOF for outlier detection
lof = LocalOutlierFactor()
outlier_scores = lof.fit_predict(baseline_sklearn_data)
outliers_mask = (outlier_scores == -1)
## Removing outliers
X_no_outliers = baseline_sklearn_data[~outliers_mask]
y_no_outliers = y[~outliers_mask]
# #Use a ratio of 70% training - 30% testing
X_train, X_test, y_train, y_test = train_test_split(X_no_outliers, y_no_outliers, test_size=0.3, random_state=1)
sk_dt = DTC()
sk_dt.fit(X_train, y_train)
predictions = sk_dt.predict(X_test)
print(classification_report(y_test, predictions))
cm = confusion_matrix(y_test, predictions)
print(cm)

Hyperparameter Tuning of the Decision Tree
baseline_sklearn_data = X.copy()
num_cols = baseline_sklearn_data.select_dtypes(include=['int64', 'float64']).columns
baseline_sklearn_data[num_cols] = baseline_sklearn_data[num_cols].apply(lambda col: col.fillna(0, axis=0))
obj_cols = baseline_sklearn_data.select_dtypes(include=['object', 'category']).columns
baseline_sklearn_data[obj_cols] = baseline_sklearn_data[obj_cols].apply(lambda col: col.fillna('?', axis=0))
baseline_sklearn_data[obj_cols] = baseline_sklearn_data[obj_cols].apply(LabelEncoder().fit_transform)
baseline_sklearn_data.sample(10)
X_train, X_test, y_train, y_test = train_test_split(baseline_sklearn_data, y, test_size=0.3, random_state=1)
## Criterion
dtc = DTC(criterion='entropy', random_state=1)
dtc.fit(X_train, y_train)
predictions = dtc.predict(X_test)
print(classification_report(y_test, predictions))
cm = confusion_matrix(y_test, predictions)
print(cm)

baseline_sklearn_data = X.copy()
num_cols = baseline_sklearn_data.select_dtypes(include=['int64', 'float64']).columns
baseline_sklearn_data[num_cols] = baseline_sklearn_data[num_cols].apply(lambda col: col.fillna(0, axis=0))
obj_cols = baseline_sklearn_data.select_dtypes(include=['object', 'category']).columns
baseline_sklearn_data[obj_cols] = baseline_sklearn_data[obj_cols].apply(lambda col: col.fillna('?', axis=0))
baseline_sklearn_data[obj_cols] = baseline_sklearn_data[obj_cols].apply(LabelEncoder().fit_transform)
baseline_sklearn_data.sample(10)
X_train, X_test, y_train, y_test = train_test_split(baseline_sklearn_data, y, test_size=0.3, random_state=1)
## changin the impurity threshold 
dtc = DTC(min_impurity_decrease=0.01, random_state=1);
dtc.fit(X_train, y_train)
predictions = dtc.predict(X_test)
print(classification_report(y_test, predictions))
cm = confusion_matrix(y_test, predictions)
print(cm)

baseline_sklearn_data = X.copy()
num_cols = baseline_sklearn_data.select_dtypes(include=['int64', 'float64']).columns
baseline_sklearn_data[num_cols] = baseline_sklearn_data[num_cols].apply(lambda col: col.fillna(0, axis=0))
obj_cols = baseline_sklearn_data.select_dtypes(include=['object', 'category']).columns
baseline_sklearn_data[obj_cols] = baseline_sklearn_data[obj_cols].apply(lambda col: col.fillna('?', axis=0))
baseline_sklearn_data[obj_cols] = baseline_sklearn_data[obj_cols].apply(LabelEncoder().fit_transform)
baseline_sklearn_data.sample(10)
X_train, X_test, y_train, y_test = train_test_split(baseline_sklearn_data, y, test_size=0.3, random_state=1)

## change the depth of the tree
dtc = DTC(max_depth=5, random_state=1);
dtc.fit(X_train, y_train)

predictions = dtc.predict(X_test)
print(classification_report(y_test, predictions))
cm = confusion_matrix(y_test, predictions)
print(cm)

baseline_sklearn_data = X.copy()
num_cols = baseline_sklearn_data.select_dtypes(include=['int64', 'float64']).columns
baseline_sklearn_data[num_cols] = baseline_sklearn_data[num_cols].apply(lambda col: col.fillna(0, axis=0))
obj_cols = baseline_sklearn_data.select_dtypes(include=['object', 'category']).columns
baseline_sklearn_data[obj_cols] = baseline_sklearn_data[obj_cols].apply(lambda col: col.fillna('?', axis=0))
baseline_sklearn_data[obj_cols] = baseline_sklearn_data[obj_cols].apply(LabelEncoder().fit_transform)
baseline_sklearn_data.sample(10)
X_train, X_test, y_train, y_test = train_test_split(baseline_sklearn_data, y, test_size=0.3, random_state=1)

## Changing the number of rows or samples that is needed in spliting the internal node
dtc = DTC(min_samples_split=10, random_state=1);
dtc.fit(X_train, y_train)

predictions = dtc.predict(X_test)
print(classification_report(y_test, predictions))
cm = confusion_matrix(y_test, predictions)
print(cm)

baseline_sklearn_data = X.copy()
num_cols = baseline_sklearn_data.select_dtypes(include=['int64', 'float64']).columns
baseline_sklearn_data[num_cols] = baseline_sklearn_data[num_cols].apply(lambda col: col.fillna(0, axis=0))
obj_cols = baseline_sklearn_data.select_dtypes(include=['object', 'category']).columns
baseline_sklearn_data[obj_cols] = baseline_sklearn_data[obj_cols].apply(lambda col: col.fillna('?', axis=0))
baseline_sklearn_data[obj_cols] = baseline_sklearn_data[obj_cols].apply(LabelEncoder().fit_transform)
baseline_sklearn_data.sample(10)
X_train, X_test, y_train, y_test = train_test_split(baseline_sklearn_data, y, test_size=0.3, random_state=1)

## Changing the number of rows or samples that is needed at a leaf
dtc = DTC(min_samples_leaf=5, random_state=1); 
dtc.fit(X_train, y_train)

predictions = dtc.predict(X_test)
print(classification_report(y_test, predictions))
cm = confusion_matrix(y_test, predictions)
print(cm)

## Using the grid search to find the best set of paremeters
params = {
    'criterion': ['gini', 'entropy'],
    'max_depth': range(3, 11),
    'min_samples_split': range(5, 16),
    'min_samples_leaf': range(3, 10),
    'min_impurity_decrease': [0.01, 0.02, 0.03]
}

grid_search = GridSearchCV(DTC(random_state=1), param_grid=params, verbose=2)

grid_search.fit(X, y)
print(grid_search.best_params_)
print(grid_search.best_estimator_)
print(grid_search.best_score_)





